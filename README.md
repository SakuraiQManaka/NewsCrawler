# 新闻聚合与事件追踪系统

这是一个基于Python的新闻聚合与事件追踪系统，能够自动爬取多个新闻源的报道，使用AI技术进行内容分析和事件聚类，最终生成重要事件的总结报告。

## 文件结构

### 📁 主要文件

#### 1. `main-dev.py` - 主程序

**功能**：系统的核心控制模块，协调整个新闻处理流程

**主要功能**：

- 新闻爬取调度和管理
- AI内容分析（关键词提取、总结生成）
- 向量化事件追踪和聚类
- 数据库操作和事件管理
- 重要事件排序和输出

**核心类**：

- `NewsAITool` - 主控制器
- `NewsProcessor` - 新闻内容处理器
- `NewsDatabase` - 数据库管理器
- `VectorEventTracker` - 基于向量的事件追踪器

#### 2. `db_looker.py` - 数据库查看器

**功能**：图形化SQLite数据库查看和编辑工具

**主要特性**：

- 🔍 可视化浏览数据库表和数据
- 📊 支持表结构查看和数据展示
- ✏️ 双击单元格直接编辑数据
- 🔄 实时刷新数据库内容
- 🎯 自动检测主键并支持数据更新

**使用方法**：

bash

```
python db_looker.py
```



#### 3. `logs.py` - 日志记录模块

**功能**：JSON格式的日志记录系统

**主要特性**：

- 📝 结构化JSON日志记录
- 🎚️ 多级别日志（INFO、WARNING、ERROR、DEBUG）
- 📁 自动文件管理和日志轮转
- ⚡ 简洁的API接口

**使用方法**：

python

```
from logs import info, warning, error, debug

info("操作成功", extra_field="额外信息")
warning("警告信息")
error("错误信息", error_code=500)
```



#### 4. `newscrawler.py` - 新闻爬虫模块

**功能**：多新闻源爬取实现

**支持的新闻源**：

- 新华网（Xinhua）
- 人民日报（People Daily）

**核心类**：

- `NewsCrawler` - 爬虫基类
- `XinhuaCrawler` - 新华网爬虫
- `PeopleDailyCrawler` - 人民日报爬虫

## 🚀 快速开始

### 环境要求

- Python 3.7+
- 以下Python包：
  - `requests`
  - `beautifulsoup4`
  - `openai`
  - `scikit-learn`
  - `numpy`
  - `sqlite3`
  - `tkinter`（GUI支持）

### 安装依赖

bash

```
pip install requests beautifulsoup4 openai scikit-learn numpy
```



### 配置API密钥

在运行前需要配置以下API密钥：

- DeepSeek API密钥，模型为Deepseek-chat（设置环境变量 `DEEPSEEK_API_KEY_API_KEY`）
- DASHSCOPE API秘钥，模型为text-embedding-v4（设置环境变量 `DASHSCOPE_API_KEY`）
- 其他AI服务API密钥（根据需要使用）

### 运行系统

bash

```
python main-dev.py
```



## 🔧 系统流程

1. **新闻爬取**：从配置的新闻网站自动爬取最新报道
2. **内容处理**：使用AI提取关键词和生成文本嵌入向量
3. **事件聚类**：基于向量相似度对相关报道进行聚类
4. **总结生成**：为每个事件生成一句话新闻和详细总结
5. **权重计算**：根据报道数量和来源多样性计算事件重要性
6. **结果输出**：展示最重要的事件和相关信息

## 📊 数据库结构

系统使用SQLite数据库存储数据，包含两个主要表：

### `articles` 表

- 存储爬取的新闻文章
- 包含标题、内容、来源、发布时间、URL等信息
- 保存关键词和文本嵌入向量

### `events` 表

- 存储聚类后的事件信息
- 包含事件总结、权重、相关文章数量等

## 🛠️ 工具使用

### 数据库查看器

运行 `db_looker.py` 可以：

- 查看新闻文章和事件数据
- 编辑数据库内容
- 监控系统运行状态

### 日志系统

系统运行日志保存在 `log.json` 文件中，便于问题排查和运行监控。

## ⚠️ 注意事项

1. **API限制**：注意AI服务的调用频率和配额限制
2. **网络要求**：确保网络连接稳定，能够访问目标新闻网站
3. **法律合规**：遵守目标网站的robots.txt和使用条款
4. **数据存储**：定期备份数据库文件

## 🔮 扩展功能

系统设计支持扩展：

- 添加新的新闻源爬虫
- 调整事件聚类算法参数
- 自定义权重计算规则
- 集成更多AI服务提供商

## 📝 版本信息

当前版本：开发版 (dev)
主要功能：新闻爬取、AI分析、事件聚类、总结生成

如有问题或建议，请查看日志文件或联系开发人员。

## 📄 许可证

本项目采用 **GNU Affero General Public License v3.0 (AGPL-3.0)** 许可证。

### 重要条款
- ✅ **允许**：查看、使用、修改、分发源代码
- ✅ **要求**：修改版本必须**开源**并保持相同许可证
- ✅ **要求**：必须**保留版权声明**和出处说明
- ⚠️ **限制**：商业使用需要获得授权
- 🌐 **网络服务**：如果作为网络服务提供，必须提供源代码

### 商业授权
如需商业使用，请联系作者获取商业授权。

完整许可证条款详见 [LICENSE](LICENSE) 文件。

*readme文档由ai生成*
